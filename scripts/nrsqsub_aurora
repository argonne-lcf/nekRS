#!/bin/bash
set -e

#--------------------------------------
: ${QUEUE:="EarlyAppAccess"}
: ${NEKRS_GPU_MPI:=0}
: ${NEKRS_BACKEND:="dpcpp"}
: ${RANKS_PER_NODE:=12}
: ${RANKS_FOR_BUILD:=12}
: ${CPU_BIND_LIST:="0-7,104-111:8-15,112-119:16-23,120-127:24-31,128-135:32-39,136-143:40-47,144-151:52-59,156-163:60-67,164-171:68-75,172-179:76-83,180-187:84-91,188-195:92-99,196-203"}
: ${OCCA_DPCPP_COMPILER_FLAGS:="-O3 -fsycl -fsycl-targets=intel_gpu_pvc -ftarget-register-alloc-mode=pvc:auto -fma"}
: ${ONEAPI_SDK:="oneapi/eng-compiler/2024.07.30.002"}
#--------------------------------------

source $NEKRS_HOME/bin/nrsqsub_utils
setup $# 1

TOTAL_RANKS=$(( nodes * RANKS_PER_NODE ))
gpus_per_node=6
tiles_per_gpu=2

chk_case $TOTAL_RANKS

#.. ----------------------------------
#.. if your job hangs when you restart your simulation:
#.. try one of the striping_unit(s)
#striping_unit=16777216
striping_unit=$((1024*1024))
#.. if your job hangs when you restart your simulation:
#.. try one of the max_striping_factor(s)
#max_striping_factor=48
max_striping_factor=56
set +e; let striping_factor=$nodes/2; set -e
if [ $striping_factor -gt $max_striping_factor ]; then
  striping_factor=$max_striping_factor
fi
if [ $striping_factor -lt 1 ]; then
  striping_factor=1
fi

MPICH_MPIIO_HINTS="*:striping_unit=${striping_unit}:striping_factor=${striping_factor}:romio_cb_write=enable:romio_ds_write=disable:romio_no_indep_rw=true"
#.. ----------------------------------

#--------------------------------------
# Generate the submission script
SFILE=s.bin
echo "#!/bin/bash" > $SFILE
echo "#PBS -A $PROJ_ID" >>$SFILE
#echo "#PBS -N $jobname" >>$SFILE
echo "#PBS -N Phlough3D" >>$SFILE
echo "#PBS -l walltime=${time}:00" >>$SFILE
echo "#PBS -l filesystems=home:flare" >>$SFILE
echo "#PBS -l select=$qnodes" >>$SFILE
echo "#PBS -l place=scatter" >>$SFILE
echo "#PBS -k doe" >>$SFILE
echo "#PBS -j oe" >>$SFILE

echo "export TZ='/usr/share/zoneinfo/US/Central'" >> $SFILE

# job to "run" from your submission directory
echo "cd \$PBS_O_WORKDIR" >> $SFILE

echo "echo Jobid: \$PBS_JOBID" >>$SFILE
echo "echo Running on host \`hostname\`" >>$SFILE
echo "echo Running on nodes \`cat \$PBS_NODEFILE\`" >>$SFILE

echo "module reload" >> $SFILE
if [ -n "$ONEAPI_SDK" ]; then
  echo "module load ${ONEAPI_SDK}" >> $SFILE
fi
echo "module load cmake" >> $SFILE
echo "module list" >> $SFILE

echo "export NEKRS_HOME=$NEKRS_HOME" >>$SFILE
echo "export NEKRS_GPU_MPI=$NEKRS_GPU_MPI" >>$SFILE
echo "export MPICH_GPU_SUPPORT_ENABLED=$NEKRS_GPU_MPI" >> $SFILE

echo "export MPICH_MPIIO_HINTS=$MPICH_MPIIO_HINTS" >>$SFILE
echo "export MPICH_MPIIO_STATS=1" >>$SFILE

echo "export OCCA_DPCPP_COMPILER_FLAGS=\"$OCCA_DPCPP_COMPILER_FLAGS\"" >> $SFILE

#.. new environment variables after teh October 29th UMD upgrade
echo "export MPIR_CVAR_CH4_SHM_POSIX_TOPO_ENABLE=1" >> $SFILE
echo "export MPIR_CVAR_INIT_SKIP_PMI_BARRIER=0" >> $SFILE

# Cray MPI libfabric defaults
# .. While the MPIIO_HINTS are important
# .. its not clear if any of these environment variables help at all
echo "export FI_CXI_RDZV_THRESHOLD=16384" >> $SFILE
echo "export FI_CXI_RDZV_EAGER_SIZE=2048" >> $SFILE
echo "export FI_CXI_DEFAULT_CQ_SIZE=131072" >> $SFILE
echo "export FI_CXI_DEFAULT_TX_SIZE=1024" >> $SFILE
echo "export FI_CXI_OFLOW_BUF_SIZE=12582912" >> $SFILE
echo "export FI_CXI_OFLOW_BUF_COUNT=3" >> $SFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $SFILE
echo "export FI_CXI_REQ_BUF_SIZE=12582912" >> $SFILE
echo "export FI_MR_CACHE_MAX_SIZE=-1" >> $SFILE
echo "export FI_MR_CACHE_MAX_COUNT=524288" >> $SFILE
echo "export FI_CXI_REQ_BUF_MAX_CACHED=0" >> $SFILE
echo "export FI_CXI_REQ_BUF_MIN_POSTED=6" >> $SFILE
# echo "export FI_CXI_RX_MATCH_MODE=hardware" >> $SFILE
echo "export FI_CXI_RX_MATCH_MODE=hybrid" >> $SFILE # required by parRSB

#.. the following will take effect only if MPICH is compiled with this option
#echo "export MPIR_CVAR_DEBUG_PROGRESS_TIMEOUT=20" >> $SFILE

CMD=.lhelper
echo "#!/bin/bash" > $CMD
echo "gpu_id=\$(((PALS_LOCAL_RANKID / ${tiles_per_gpu}) % ${gpus_per_node}))" >> $CMD
echo "tile_id=\$((PALS_LOCAL_RANKID % ${tiles_per_gpu}))" >> $CMD
echo "export ZE_AFFINITY_MASK=\$gpu_id.\$tile_id" >> $CMD
echo "\"\$@\"" >> $CMD
chmod u+x $CMD

if [ $RUN_ONLY -eq 0 ]; then
  echo -e "\n# precompilation" >>$SFILE
  CMD_build="mpiexec --no-vni -n ${RANKS_FOR_BUILD} -ppn ${RANKS_FOR_BUILD} --cpu-bind=list:${CPU_BIND_LIST} -- ./${CMD} $bin --setup \${case_tmp} --backend ${NEKRS_BACKEND} --device-id 0 $extra_args --build-only \${ntasks_tmp}"
  add_build_CMD "$SFILE" "$CMD_build" "$TOTAL_RANKS"
fi

if [ $BUILD_ONLY -eq 0 ]; then
  link_neknek_logfile "$SFILE"
  echo -e "\n# actual run" >>$SFILE
  echo "mpiexec --no-vni -n ${TOTAL_RANKS} -ppn ${RANKS_PER_NODE} --cpu-bind=list:${CPU_BIND_LIST} -- ./${CMD} $bin --setup ${case} --backend ${NEKRS_BACKEND} --device-id 0 $extra_args" >> $SFILE
fi

qsub -q $QUEUE $SFILE
